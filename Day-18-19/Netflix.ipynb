{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b13e67eb-708b-452b-bb33-06231240991a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titles_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"inferSchema\",\"true\") \\\n",
    "    .load(\"/Volumes/workspace/default/netflix/titles.csv\") \n",
    "titles_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"titles_df\")\n",
    "titles_df = spark.read.table(\"titles_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f528646c-6409-4b6e-ad20-ec339df514b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "credits_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"inferSchema\",\"true\") \\\n",
    "    .load(\"/Volumes/workspace/default/netflix/credits.csv\") \n",
    "credits_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"credits_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b87f7c7-8698-494f-a5fb-dde3f67cc33f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "credits_df = spark.read.table(\"credits_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0394ee47-704e-408b-9e60-d420b5f556e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('id', 'string'), ('title', 'string'), ('type', 'string'), ('description', 'string'), ('release_year', 'string'), ('age_certification', 'string'), ('runtime', 'string'), ('genres', 'string'), ('production_countries', 'string'), ('seasons', 'string'), ('imdb_id', 'string'), ('imdb_score', 'string'), ('imdb_votes', 'string'), ('tmdb_popularity', 'string'), ('tmdb_score', 'string')]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>title</th><th>type</th><th>description</th><th>release_year</th><th>age_certification</th><th>runtime</th><th>genres</th><th>production_countries</th><th>seasons</th><th>imdb_id</th><th>imdb_score</th><th>imdb_votes</th><th>tmdb_popularity</th><th>tmdb_score</th></tr></thead><tbody><tr><td>tm84618</td><td>Taxi Driver</td><td>MOVIE</td><td>A mentally unstable Vietnam War veteran works as a night-time taxi driver in New York City where the perceived decadence and sleaze feed his urge for violent action.</td><td>1976</td><td>R</td><td>114</td><td>['drama', 'crime']</td><td>['US']</td><td>null</td><td>tt0075314</td><td>8.2</td><td>808582.0</td><td>40.965</td><td>8.179</td></tr><tr><td>tm154986</td><td>Deliverance</td><td>MOVIE</td><td>Intent on seeing the Cahulawassee River before it's turned into one huge lake, outdoor fanatic Lewis Medlock takes his friends on a river-rafting trip they'll never forget into the dangerous American back-country.</td><td>1972</td><td>R</td><td>109</td><td>['drama', 'action', 'thriller', 'european']</td><td>['US']</td><td>null</td><td>tt0068473</td><td>7.7</td><td>107673.0</td><td>10.01</td><td>7.3</td></tr><tr><td>tm127384</td><td>Monty Python and the Holy Grail</td><td>MOVIE</td><td>\"King Arthur, accompanied by his squire, recruits his Knights of the Round Table, including Sir Bedevere the Wise, Sir Lancelot the Brave, Sir Robin the Not-Quite-So-Brave-As-Sir-Lancelot and Sir Galahad the Pure. On the way, Arthur battles the Black Knight who, despite having had all his limbs chopped off, insists he can still fight. They reach Camelot, but Arthur decides not  to enter, as \"\"it is a silly place\"\".\"</td><td>1975</td><td>PG</td><td>91</td><td>['fantasy', 'action', 'comedy']</td><td>['GB']</td><td>null</td><td>tt0071853</td><td>8.2</td><td>534486.0</td><td>15.461</td><td>7.811</td></tr><tr><td>tm120801</td><td>The Dirty Dozen</td><td>MOVIE</td><td>12 American military prisoners in World War II are ordered to infiltrate a well-guarded enemy château and kill the Nazi officers vacationing there. The soldiers, most of whom are facing death sentences for a variety of violent crimes, agree to the mission and the possible commuting of their sentences.</td><td>1967</td><td>TV-MA</td><td>150</td><td>['war', 'action']</td><td>['GB', 'US']</td><td>null</td><td>tt0061578</td><td>7.7</td><td>72662.0</td><td>20.398</td><td>7.6</td></tr><tr><td>ts22164</td><td>Monty Python's Flying Circus</td><td>SHOW</td><td>A British sketch comedy series with the shows being composed of surreality, risqué or innuendo-laden humour, sight gags and observational sketches without punchlines.</td><td>1969</td><td>TV-14</td><td>30</td><td>['comedy', 'european']</td><td>['GB']</td><td>4.0</td><td>tt0063929</td><td>8.8</td><td>73424.0</td><td>17.617</td><td>8.306</td></tr><tr><td>tm70993</td><td>Life of Brian</td><td>MOVIE</td><td>Brian Cohen is an average young Jewish man, but through a series of ridiculous events, he gains a reputation as the Messiah. When he's not dodging his followers or being scolded by his shrill mother, the hapless Brian has to contend with the pompous Pontius Pilate and acronym-obsessed members of a separatist movement. Rife with Monty Python's signature absurdity, the tale finds Brian's life paralleling Biblical lore, albeit with many more laughs.</td><td>1979</td><td>R</td><td>94</td><td>['comedy']</td><td>['GB']</td><td>null</td><td>tt0079470</td><td>8.0</td><td>395024.0</td><td>17.77</td><td>7.8</td></tr><tr><td>tm14873</td><td>Dirty Harry</td><td>MOVIE</td><td>When a madman dubbed 'Scorpio' terrorizes San Francisco, hard-nosed cop, Harry Callahan – famous for his take-no-prisoners approach to law enforcement – is tasked with hunting down the psychopath. Harry eventually collars Scorpio in the process of rescuing a kidnap victim, only to see him walk on technicalities. Now, the maverick detective is determined to nail the maniac himself.</td><td>1971</td><td>R</td><td>102</td><td>['thriller', 'action', 'crime']</td><td>['US']</td><td>null</td><td>tt0066999</td><td>7.7</td><td>155051.0</td><td>12.817</td><td>7.5</td></tr><tr><td>tm119281</td><td>Bonnie and Clyde</td><td>MOVIE</td><td>In the 1930s, bored waitress Bonnie Parker falls in love with an ex-con named Clyde Barrow and together they start a violent crime spree through the country, stealing cars and robbing banks.</td><td>1967</td><td>R</td><td>110</td><td>['crime', 'drama', 'action']</td><td>['US']</td><td>null</td><td>tt0061418</td><td>7.7</td><td>112048.0</td><td>15.687</td><td>7.5</td></tr><tr><td>tm98978</td><td>The Blue Lagoon</td><td>MOVIE</td><td>Two small children and a ship's cook survive a shipwreck and find safety on an idyllic tropical island. Soon, however, the cook dies and the young boy and girl are left on their own. Days become years and Emmeline and Richard make a home for themselves surrounded by exotic creatures and nature's beauty. But will they ever see civilization again?</td><td>1980</td><td>R</td><td>104</td><td>['romance', 'action', 'drama']</td><td>['US']</td><td>null</td><td>tt0080453</td><td>5.8</td><td>69844.0</td><td>50.324</td><td>6.156</td></tr><tr><td>tm44204</td><td>The Guns of Navarone</td><td>MOVIE</td><td>A team of allied saboteurs are assigned an impossible mission: infiltrate an impregnable Nazi-held island and destroy the two enormous long-range field guns that prevent the rescue of 2,000 trapped British soldiers.</td><td>1961</td><td>TV-MA</td><td>158</td><td>['action', 'drama', 'war']</td><td>['GB', 'US']</td><td>null</td><td>tt0054953</td><td>7.5</td><td>50748.0</td><td>13.844</td><td>7.3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "tm84618",
         "Taxi Driver",
         "MOVIE",
         "A mentally unstable Vietnam War veteran works as a night-time taxi driver in New York City where the perceived decadence and sleaze feed his urge for violent action.",
         "1976",
         "R",
         "114",
         "['drama', 'crime']",
         "['US']",
         null,
         "tt0075314",
         "8.2",
         "808582.0",
         "40.965",
         "8.179"
        ],
        [
         "tm154986",
         "Deliverance",
         "MOVIE",
         "Intent on seeing the Cahulawassee River before it's turned into one huge lake, outdoor fanatic Lewis Medlock takes his friends on a river-rafting trip they'll never forget into the dangerous American back-country.",
         "1972",
         "R",
         "109",
         "['drama', 'action', 'thriller', 'european']",
         "['US']",
         null,
         "tt0068473",
         "7.7",
         "107673.0",
         "10.01",
         "7.3"
        ],
        [
         "tm127384",
         "Monty Python and the Holy Grail",
         "MOVIE",
         "\"King Arthur, accompanied by his squire, recruits his Knights of the Round Table, including Sir Bedevere the Wise, Sir Lancelot the Brave, Sir Robin the Not-Quite-So-Brave-As-Sir-Lancelot and Sir Galahad the Pure. On the way, Arthur battles the Black Knight who, despite having had all his limbs chopped off, insists he can still fight. They reach Camelot, but Arthur decides not  to enter, as \"\"it is a silly place\"\".\"",
         "1975",
         "PG",
         "91",
         "['fantasy', 'action', 'comedy']",
         "['GB']",
         null,
         "tt0071853",
         "8.2",
         "534486.0",
         "15.461",
         "7.811"
        ],
        [
         "tm120801",
         "The Dirty Dozen",
         "MOVIE",
         "12 American military prisoners in World War II are ordered to infiltrate a well-guarded enemy château and kill the Nazi officers vacationing there. The soldiers, most of whom are facing death sentences for a variety of violent crimes, agree to the mission and the possible commuting of their sentences.",
         "1967",
         "TV-MA",
         "150",
         "['war', 'action']",
         "['GB', 'US']",
         null,
         "tt0061578",
         "7.7",
         "72662.0",
         "20.398",
         "7.6"
        ],
        [
         "ts22164",
         "Monty Python's Flying Circus",
         "SHOW",
         "A British sketch comedy series with the shows being composed of surreality, risqué or innuendo-laden humour, sight gags and observational sketches without punchlines.",
         "1969",
         "TV-14",
         "30",
         "['comedy', 'european']",
         "['GB']",
         "4.0",
         "tt0063929",
         "8.8",
         "73424.0",
         "17.617",
         "8.306"
        ],
        [
         "tm70993",
         "Life of Brian",
         "MOVIE",
         "Brian Cohen is an average young Jewish man, but through a series of ridiculous events, he gains a reputation as the Messiah. When he's not dodging his followers or being scolded by his shrill mother, the hapless Brian has to contend with the pompous Pontius Pilate and acronym-obsessed members of a separatist movement. Rife with Monty Python's signature absurdity, the tale finds Brian's life paralleling Biblical lore, albeit with many more laughs.",
         "1979",
         "R",
         "94",
         "['comedy']",
         "['GB']",
         null,
         "tt0079470",
         "8.0",
         "395024.0",
         "17.77",
         "7.8"
        ],
        [
         "tm14873",
         "Dirty Harry",
         "MOVIE",
         "When a madman dubbed 'Scorpio' terrorizes San Francisco, hard-nosed cop, Harry Callahan – famous for his take-no-prisoners approach to law enforcement – is tasked with hunting down the psychopath. Harry eventually collars Scorpio in the process of rescuing a kidnap victim, only to see him walk on technicalities. Now, the maverick detective is determined to nail the maniac himself.",
         "1971",
         "R",
         "102",
         "['thriller', 'action', 'crime']",
         "['US']",
         null,
         "tt0066999",
         "7.7",
         "155051.0",
         "12.817",
         "7.5"
        ],
        [
         "tm119281",
         "Bonnie and Clyde",
         "MOVIE",
         "In the 1930s, bored waitress Bonnie Parker falls in love with an ex-con named Clyde Barrow and together they start a violent crime spree through the country, stealing cars and robbing banks.",
         "1967",
         "R",
         "110",
         "['crime', 'drama', 'action']",
         "['US']",
         null,
         "tt0061418",
         "7.7",
         "112048.0",
         "15.687",
         "7.5"
        ],
        [
         "tm98978",
         "The Blue Lagoon",
         "MOVIE",
         "Two small children and a ship's cook survive a shipwreck and find safety on an idyllic tropical island. Soon, however, the cook dies and the young boy and girl are left on their own. Days become years and Emmeline and Richard make a home for themselves surrounded by exotic creatures and nature's beauty. But will they ever see civilization again?",
         "1980",
         "R",
         "104",
         "['romance', 'action', 'drama']",
         "['US']",
         null,
         "tt0080453",
         "5.8",
         "69844.0",
         "50.324",
         "6.156"
        ],
        [
         "tm44204",
         "The Guns of Navarone",
         "MOVIE",
         "A team of allied saboteurs are assigned an impossible mission: infiltrate an impregnable Nazi-held island and destroy the two enormous long-range field guns that prevent the rescue of 2,000 trapped British soldiers.",
         "1961",
         "TV-MA",
         "158",
         "['action', 'drama', 'war']",
         "['GB', 'US']",
         null,
         "tt0054953",
         "7.5",
         "50748.0",
         "13.844",
         "7.3"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "release_year",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age_certification",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "runtime",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "genres",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "production_countries",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "seasons",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "imdb_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "imdb_score",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "imdb_votes",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tmdb_popularity",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tmdb_score",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(titles_df.dtypes)\n",
    "display(titles_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b68635f6-8046-40be-8960-8226c13a4e4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>title</th><th>type</th><th>description</th><th>release_year</th><th>age_certification</th><th>runtime</th><th>genres</th><th>production_countries</th><th>seasons</th><th>imdb_id</th><th>imdb_score</th><th>imdb_votes</th><th>tmdb_popularity</th><th>tmdb_score</th></tr></thead><tbody><tr><td>0</td><td>3</td><td>18</td><td>40</td><td>70</td><td>2594</td><td>121</td><td>97</td><td>96</td><td>3684</td><td>541</td><td>575</td><td>600</td><td>223</td><td>418</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         3,
         18,
         40,
         70,
         2594,
         121,
         97,
         96,
         3684,
         541,
         575,
         600,
         223,
         418
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "title",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "type",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "description",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "release_year",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "age_certification",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "runtime",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "genres",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "production_countries",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "seasons",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "imdb_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "imdb_score",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "imdb_votes",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "tmdb_popularity",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "tmdb_score",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnull, count, when\n",
    "nulls = titles_df.select([count(when(isnull(c),c)).alias(c) for c in titles_df.columns])\n",
    "nulls.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bf69b8e-4ff0-421b-a983-fe19f3dab193",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "critical_col = ['id', 'title', 'release_year', 'imdb_id']\n",
    "titles_df = titles_df.dropna(subset=critical_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ff13adc-4d02-4625-bd93-1655ec0f3a17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titles_df = titles_df.fillna(\"No Description\", \"description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c32aa090-d27d-4886-97d6-2f3ede806296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "ty = [\"MOVIE\",\"SHOW\"]\n",
    "titles_df = titles_df.fillna(random.choice(ty),\"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a7fd3b-ebf3-4391-90f0-49b3ecee5963",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TV-MA\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "mode = (titles_df.filter(col(\"age_certification\").isNotNull()).groupBy(\"age_certification\").count().orderBy(desc(\"count\")).first()[\"age_certification\"])\n",
    "print(mode)\n",
    "titles_df = titles_df.fillna(mode,\"age_certification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15e7fc9e-26d4-4e63-9603-29a48655632d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import mean\n",
    "# mean_runtime = titles_df.select(mean(\"runtime\")).first()[0]\n",
    "# print(mean_runtime)\n",
    "# titles_df = titles_df.fillna(mean_runtime, \"runtime\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73804dfe-00e4-4806-803c-bdaeff7f638c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df = titles_df.join(credits_df, on=\"id\", how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4e6291a-dbdc-4136-8718-fa1f39adccee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----+--------------------+------------+-----------------+-------+------------------+--------------------+-------+---------+----------+----------+---------------+----------+---------+------------------+--------------------+-----+\n|     id|      title| type|         description|release_year|age_certification|runtime|            genres|production_countries|seasons|  imdb_id|imdb_score|imdb_votes|tmdb_popularity|tmdb_score|person_id|              name|           character| role|\n+-------+-----------+-----+--------------------+------------+-----------------+-------+------------------+--------------------+-------+---------+----------+----------+---------------+----------+---------+------------------+--------------------+-----+\n|tm84618|Taxi Driver|MOVIE|A mentally unstab...|        1976|                R|    114|['drama', 'crime']|              ['US']|   NULL|tt0075314|       8.2|  808582.0|         40.965|     8.179|     3748|    Robert De Niro|       Travis Bickle|ACTOR|\n|tm84618|Taxi Driver|MOVIE|A mentally unstab...|        1976|                R|    114|['drama', 'crime']|              ['US']|   NULL|tt0075314|       8.2|  808582.0|         40.965|     8.179|    14658|      Jodie Foster|       Iris Steensma|ACTOR|\n|tm84618|Taxi Driver|MOVIE|A mentally unstab...|        1976|                R|    114|['drama', 'crime']|              ['US']|   NULL|tt0075314|       8.2|  808582.0|         40.965|     8.179|     7064|     Albert Brooks|                 Tom|ACTOR|\n|tm84618|Taxi Driver|MOVIE|A mentally unstab...|        1976|                R|    114|['drama', 'crime']|              ['US']|   NULL|tt0075314|       8.2|  808582.0|         40.965|     8.179|     3739|     Harvey Keitel|Matthew 'Sport' H...|ACTOR|\n|tm84618|Taxi Driver|MOVIE|A mentally unstab...|        1976|                R|    114|['drama', 'crime']|              ['US']|   NULL|tt0075314|       8.2|  808582.0|         40.965|     8.179|    48933|   Cybill Shepherd|               Betsy|ACTOR|\n|tm84618|Taxi Driver|MOVIE|A mentally unstab...|        1976|                R|    114|['drama', 'crime']|              ['US']|   NULL|tt0075314|       8.2|  808582.0|         40.965|     8.179|    32267|       Peter Boyle|              Wizard|ACTOR|\n|tm84618|Taxi Driver|MOVIE|A mentally unstab...|        1976|                R|    114|['drama', 'crime']|              ['US']|   NULL|tt0075314|       8.2|  808582.0|         40.965|     8.179|   519612|    Leonard Harris|Senator Charles P...|ACTOR|\n|tm84618|Taxi Driver|MOVIE|A mentally unstab...|        1976|                R|    114|['drama', 'crime']|              ['US']|   NULL|tt0075314|       8.2|  808582.0|         40.965|     8.179|    29068|    Diahnne Abbott|     Concession Girl|ACTOR|\n|tm84618|Taxi Driver|MOVIE|A mentally unstab...|        1976|                R|    114|['drama', 'crime']|              ['US']|   NULL|tt0075314|       8.2|  808582.0|         40.965|     8.179|   519613|       Gino Ardito|  Policeman at Rally|ACTOR|\n|tm84618|Taxi Driver|MOVIE|A mentally unstab...|        1976|                R|    114|['drama', 'crime']|              ['US']|   NULL|tt0075314|       8.2|  808582.0|         40.965|     8.179|     3308|   Martin Scorsese|Passenger Watchin...|ACTOR|\n|tm84618|Taxi Driver|MOVIE|A mentally unstab...|        1976|                R|    114|['drama', 'crime']|              ['US']|   NULL|tt0075314|       8.2|  808582.0|         40.965|     8.179|    43791|     Murray Moston|   Iris' Time Keeper|ACTOR|\n|tm84618|Taxi Driver|MOVIE|A mentally unstab...|        1976|                R|    114|['drama', 'crime']|              ['US']|   NULL|tt0075314|       8.2|  808582.0|         40.965|     8.179|   519614|     Richard Higgs|Secret Service Agent|ACTOR|\n|tm84618|Taxi Driver|MOVIE|A mentally unstab...|        1976|                R|    114|['drama', 'crime']|              ['US']|   NULL|tt0075314|       8.2|  808582.0|         40.965|     8.179|   519615|       Bill Minkin|Tom's Assistant (...|ACTOR|\n|tm84618|Taxi Driver|MOVIE|A mentally unstab...|        1976|                R|    114|['drama', 'crime']|              ['US']|   NULL|tt0075314|       8.2|  808582.0|         40.965|     8.179|    82426|        Bob Maroff|Mafioso (uncredited)|ACTOR|\n|tm84618|Taxi Driver|MOVIE|A mentally unstab...|        1976|                R|    114|['drama', 'crime']|              ['US']|   NULL|tt0075314|       8.2|  808582.0|         40.965|     8.179|    20935|       Victor Argo|Melio, Delicatess...|ACTOR|\n|tm84618|Taxi Driver|MOVIE|A mentally unstab...|        1976|                R|    114|['drama', 'crime']|              ['US']|   NULL|tt0075314|       8.2|  808582.0|         40.965|     8.179|     7753|       Joe Spinell|   Personell Officer|ACTOR|\n|tm84618|Taxi Driver|MOVIE|A mentally unstab...|        1976|                R|    114|['drama', 'crime']|              ['US']|   NULL|tt0075314|       8.2|  808582.0|         40.965|     8.179|    43279|Robinson Frank Adu|Angry Black Man (...|ACTOR|\n|tm84618|Taxi Driver|MOVIE|A mentally unstab...|        1976|                R|    114|['drama', 'crime']|              ['US']|   NULL|tt0075314|       8.2|  808582.0|         40.965|     8.179|   519616|    Brenda Dickson|Soap Opera Woman ...|ACTOR|\n|tm84618|Taxi Driver|MOVIE|A mentally unstab...|        1976|                R|    114|['drama', 'crime']|              ['US']|   NULL|tt0075314|       8.2|  808582.0|         40.965|     8.179|     8424|    Norman Matlock|           Charlie T|ACTOR|\n|tm84618|Taxi Driver|MOVIE|A mentally unstab...|        1976|                R|    114|['drama', 'crime']|              ['US']|   NULL|tt0075314|       8.2|  808582.0|         40.965|     8.179|    20447|     Harry Northup|            Doughboy|ACTOR|\n+-------+-----------+-----+--------------------+------------+-----------------+-------+------------------+--------------------+-------+---------+----------+----------+---------------+----------+---------+------------------+--------------------+-----+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dec6270-bf2d-4308-97e5-b177d00957dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n|                name|count|\n+--------------------+-----+\n|         Boman Irani|   25|\n| Kareena Kapoor Khan|   25|\n|      Shah Rukh Khan|   23|\n|    Takahiro Sakurai|   21|\n|         Raúl Campos|   20|\n|        Paresh Rawal|   20|\n|    Amitabh Bachchan|   20|\n|Priyanka Chopra J...|   20|\n|           Jan Suter|   19|\n|          Aamir Khan|   19|\n+--------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "credits_df.groupBy(\"name\") \\\n",
    "    .count().orderBy(\"count\", ascending=False).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea076e7c-794f-4fde-9cd9-0740bdefd018",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n|                type|count|\n+--------------------+-----+\n| which broke the ...|    1|\n| west against the...|    1|\n| and their improv...|    1|\n|               TV-14|    5|\n|                NULL|   18|\n| she witnesses by...|    1|\n| which is made by...|    1|\n|   including Spanish|    1|\n| a children's str...|    1|\n| one thing is cle...|    1|\n| and was eventual...|    1|\n| classmates and t...|    1|\n|                2019|    1|\n|                2017|    1|\n| cast out of the ...|    1|\n| which is driving...|    1|\n|              ['US']|    1|\n|               MOVIE| 3744|\n|         the Octopod|    1|\n|          two courts|    1|\n+--------------------+-----+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "titles_df.groupBy(\"type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24e04b3c-7ed9-423c-8629-99a2804d7a55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n|               title|imdb_score|\n+--------------------+----------+\n|ReMastered: The L...| tt9046576|\n|        Chaman Bahar| tt8747450|\n|Dance Dreams: Hot...| tt8741182|\n|Derren Brown: Mir...| tt8599562|\n|W. Kamau Bell: Pr...| tt8467788|\n|D.L. Hughley: Con...| tt8426270|\n|John Mulaney: Kid...| tt8271714|\n|No One Will Ever ...| tt8251336|\n|Agustín Aristarán...| tt8004552|\n|Ronnie Coleman: T...| tt7217214|\n+--------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "titles_df.filter(\"type = 'MOVIE' and imdb_score is not null\") \\\n",
    "    .orderBy(\"imdb_score\", ascending=False).select(\"title\", \"imdb_score\").limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "540606f5-4e2e-4d66-86f1-6419078bcdcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n|        release_year|count|\n+--------------------+-----+\n|                NULL|   70|\n| \"\"Married...\"\" f...|    1|\n| \"\"Sound of your ...|    1|\n| 2010 after the V...|    1|\n| Abdus Salam. The...|    1|\n| Autumn's Concert...|    1|\n| Barbie’s precoci...|    1|\n| Bert Kreischer: ...|    1|\n| Birbiglia tiptoe...|    1|\n| Chaldea issues a...|    1|\n|              Cheryl|    1|\n| David visits the...|    1|\n| Dennō Coil was h...|    1|\n|             Italian|    1|\n| James. Soon the ...|    1|\n| Jim's obsession ...|    1|\n| Jr. and Zen Mast...|    1|\n|              Julian|    1|\n| Kagome locates a...|    1|\n|              Lisbon|    1|\n+--------------------+-----+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "titles_df.groupBy(\"release_year\").count().orderBy(\"release_year\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00325404-831a-47db-b71a-1bb46a8d86e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n|                name|count|\n+--------------------+-----+\n| Kareena Kapoor Khan|   25|\n|         Boman Irani|   25|\n|      Shah Rukh Khan|   23|\n|    Takahiro Sakurai|   21|\n|Priyanka Chopra J...|   20|\n|        Paresh Rawal|   20|\n|    Amitabh Bachchan|   20|\n| Nawazuddin Siddiqui|   19|\n|         Anupam Kher|   19|\n|      Junichi Suwabe|   19|\n+--------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "credits_df.filter(\"role = 'ACTOR'\").groupBy(\"name\") \\\n",
    "    .count().orderBy(\"count\", ascending=False).limit(10).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d44efa31-55a1-4374-92e1-26b213cae5bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n|               title|        release_year|imdb_score|\n+--------------------+--------------------+----------+\n|         Taxi Driver|                1976|       8.2|\n|          GoodFellas|                1990|       8.7|\n|Once Upon a Time ...|                1984|       8.3|\n|          Awakenings| filled with awe ...|    ['US']|\n|Jim Norton: Mouth...|                2017|       7.0|\n|        The Irishman|                2019|       7.8|\n|The Irishman: In ...|                2019|      NULL|\n+--------------------+--------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "credits_df.filter(\"name = 'Robert De Niro'\") \\\n",
    "    .join(titles_df, \"id\").select(\"title\", \"release_year\", \"imdb_score\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e0884e9-c4c6-4ef0-8c79-32e1f3b04413",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6567264660263619>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m titles_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenre\u001B[39m\u001B[38;5;124m\"\u001B[39m, explode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenres\u001B[39m\u001B[38;5;124m\"\u001B[39m)) \\\n",
       "\u001B[0;32m----> 2\u001B[0m     \u001B[38;5;241m.\u001B[39mgroupBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenre\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mavg(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtmdb_score\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39morderBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mavg(tmdb_score)\u001B[39m\u001B[38;5;124m\"\u001B[39m, ascending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:536\u001B[0m, in \u001B[0;36mDataFrame.groupBy\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m    534\u001B[0m     _cols\u001B[38;5;241m.\u001B[39mappend(c)\n",
       "\u001B[1;32m    535\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(c, \u001B[38;5;28mstr\u001B[39m):\n",
       "\u001B[0;32m--> 536\u001B[0m     _cols\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m[c])\n",
       "\u001B[1;32m    537\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(c, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(c, \u001B[38;5;28mbool\u001B[39m):\n",
       "\u001B[1;32m    538\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m c \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:1860\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, item)\u001B[0m\n",
       "\u001B[1;32m   1856\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconnect\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtypes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m verify_col_name\n",
       "\u001B[1;32m   1858\u001B[0m     \u001B[38;5;66;03m# Try best to verify the column name with cached schema\u001B[39;00m\n",
       "\u001B[1;32m   1859\u001B[0m     \u001B[38;5;66;03m# If fails, fall back to the server side validation\u001B[39;00m\n",
       "\u001B[0;32m-> 1860\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_col_name(item, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mschema):\n",
       "\u001B[1;32m   1861\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mselect(item)\u001B[38;5;241m.\u001B[39misLocal()\n",
       "\u001B[1;32m   1863\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_col(item)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:2000\u001B[0m, in \u001B[0;36mDataFrame.schema\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1998\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cached_schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   1999\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m-> 2000\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cached_schema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mschema(query)\n",
       "\u001B[1;32m   2001\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m copy\u001B[38;5;241m.\u001B[39mdeepcopy(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cached_schema)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1262\u001B[0m, in \u001B[0;36mSparkConnectClient.schema\u001B[0;34m(self, plan)\u001B[0m\n",
       "\u001B[1;32m   1260\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m logger\u001B[38;5;241m.\u001B[39misEnabledFor(logging\u001B[38;5;241m.\u001B[39mINFO):\n",
       "\u001B[1;32m   1261\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSchema for plan: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_proto_to_string(plan,\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m-> 1262\u001B[0m schema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_analyze(method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mschema\u001B[39m\u001B[38;5;124m\"\u001B[39m, plan\u001B[38;5;241m=\u001B[39mplan)\u001B[38;5;241m.\u001B[39mschema\n",
       "\u001B[1;32m   1263\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   1264\u001B[0m \u001B[38;5;66;03m# Server side should populate the struct field which is the schema.\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1540\u001B[0m, in \u001B[0;36mSparkConnectClient._analyze\u001B[0;34m(self, method, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1538\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   1539\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1540\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2047\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2045\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2046\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2047\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2048\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
       "\u001B[1;32m   2049\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2149\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2134\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n",
       "\u001B[1;32m   2135\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython versions in the Spark Connect client and server are different. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   2136\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo execute user-defined functions, client and server should have the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2145\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://docs.databricks.com/en/release-notes/serverless.html.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   2146\u001B[0m                 )\n",
       "\u001B[1;32m   2147\u001B[0m             \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2149\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2150\u001B[0m                 info,\n",
       "\u001B[1;32m   2151\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2152\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2153\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2154\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2156\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(status\u001B[38;5;241m.\u001B[39mmessage) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2157\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"explode(genres)\" due to data type mismatch: The first parameter requires the (\"ARRAY\" or \"MAP\") type, however \"genres\" has the type \"STRING\". SQLSTATE: 42K09;\n",
       "'Project [id#12662, title#12663, type#12664, description#12665, release_year#12666, age_certification#12667, runtime#12668, genres#12669, production_countries#12670, seasons#12671, imdb_id#12672, imdb_score#12673, imdb_votes#12674, tmdb_popularity#12675, tmdb_score#12676, explode(genres#12669) AS genre#12693]\n",
       "+- SubqueryAlias workspace.default.titles_df\n",
       "   +- Relation workspace.default.titles_df[id#12662,title#12663,type#12664,description#12665,release_year#12666,age_certification#12667,runtime#12668,genres#12669,production_countries#12670,seasons#12671,imdb_id#12672,imdb_score#12673,imdb_votes#12674,tmdb_popularity#12675,tmdb_score#12676] parquet\n",
       "\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n",
       "\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:80)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:73)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:506)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
       "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
       "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\n",
       "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
       "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
       "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:256)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:216)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:256)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:89)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:293)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:532)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:659)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:659)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1299)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:652)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:649)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:649)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:287)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:286)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1683)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1744)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:320)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:266)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:108)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1457)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1457)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:106)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$org$apache$spark$sql$Dataset$$withPlan$1(Dataset.scala:5054)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:5054)\n",
       "\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1850)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withColumns$1(Dataset.scala:3221)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:3195)\n",
       "\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:3237)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformWithColumns(SparkConnectPlanner.scala:1516)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$7(SessionHolder.scala:614)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:630)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$5(SessionHolder.scala:613)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:611)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:185)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.transformRelation$1(SparkConnectAnalyzeHandler.scala:119)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:130)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:104)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:66)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
       "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:66)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:51)\n",
       "\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:258)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:50)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:109)\n",
       "\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:801)\n",
       "\tat grpc_shaded.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n",
       "\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:381)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:337)\n",
       "\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:537)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:337)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:336)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:349)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:329)\n",
       "\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:381)\n",
       "\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat grpc_shaded.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:351)\n",
       "\tat grpc_shaded.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n",
       "\tat grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n",
       "\tat grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:161)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:55)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n",
       "\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:115)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:91)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:158)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:161)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
       "\tat java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"explode(genres)\" due to data type mismatch: The first parameter requires the (\"ARRAY\" or \"MAP\") type, however \"genres\" has the type \"STRING\". SQLSTATE: 42K09;\n'Project [id#12662, title#12663, type#12664, description#12665, release_year#12666, age_certification#12667, runtime#12668, genres#12669, production_countries#12670, seasons#12671, imdb_id#12672, imdb_score#12673, imdb_votes#12674, tmdb_popularity#12675, tmdb_score#12676, explode(genres#12669) AS genre#12693]\n+- SubqueryAlias workspace.default.titles_df\n   +- Relation workspace.default.titles_df[id#12662,title#12663,type#12664,description#12665,release_year#12666,age_certification#12667,runtime#12668,genres#12669,production_countries#12670,seasons#12671,imdb_id#12672,imdb_score#12673,imdb_votes#12674,tmdb_popularity#12675,tmdb_score#12676] parquet\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:80)\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:73)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:506)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:256)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:216)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:256)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:133)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:293)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:532)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:659)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:659)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1299)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:652)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:649)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:649)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:287)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:286)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1683)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1744)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:320)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:266)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1457)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1457)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:106)\n\tat org.apache.spark.sql.Dataset.$anonfun$org$apache$spark$sql$Dataset$$withPlan$1(Dataset.scala:5054)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:5054)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1850)\n\tat org.apache.spark.sql.Dataset.$anonfun$withColumns$1(Dataset.scala:3221)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:3237)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformWithColumns(SparkConnectPlanner.scala:1516)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$7(SessionHolder.scala:614)\n\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:630)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$5(SessionHolder.scala:613)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:611)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:185)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.transformRelation$1(SparkConnectAnalyzeHandler.scala:119)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:130)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:104)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:66)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:66)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:51)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:258)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:50)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:109)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:801)\n\tat grpc_shaded.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:381)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:337)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:537)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:337)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:336)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:349)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:329)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:381)\n\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat grpc_shaded.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:351)\n\tat grpc_shaded.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:161)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:55)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:115)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:91)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:158)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:161)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       },
       "metadata": {
        "errorSummary": "[DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"explode(genres)\" due to data type mismatch: The first parameter requires the (\"ARRAY\" or \"MAP\") type, however \"genres\" has the type \"STRING\". SQLSTATE: 42K09"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": "42K09",
        "stackTrace": "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:80)\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:73)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:506)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:256)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:216)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:256)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:133)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:293)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:532)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:659)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:659)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1299)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:652)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:649)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:649)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:287)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:286)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1683)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1744)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:320)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:266)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1457)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1457)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:106)\n\tat org.apache.spark.sql.Dataset.$anonfun$org$apache$spark$sql$Dataset$$withPlan$1(Dataset.scala:5054)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:5054)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1850)\n\tat org.apache.spark.sql.Dataset.$anonfun$withColumns$1(Dataset.scala:3221)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:3237)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformWithColumns(SparkConnectPlanner.scala:1516)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$7(SessionHolder.scala:614)\n\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:630)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$5(SessionHolder.scala:613)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:611)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:185)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.transformRelation$1(SparkConnectAnalyzeHandler.scala:119)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:130)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:104)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:66)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:66)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:51)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:258)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:50)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:109)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:801)\n\tat grpc_shaded.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:381)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:337)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:537)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:337)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:336)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:349)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:329)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:381)\n\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat grpc_shaded.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:351)\n\tat grpc_shaded.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:161)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:55)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:115)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:91)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:158)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:161)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-6567264660263619>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m titles_df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenre\u001B[39m\u001B[38;5;124m\"\u001B[39m, explode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenres\u001B[39m\u001B[38;5;124m\"\u001B[39m)) \\\n\u001B[0;32m----> 2\u001B[0m     \u001B[38;5;241m.\u001B[39mgroupBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenre\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mavg(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtmdb_score\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39morderBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mavg(tmdb_score)\u001B[39m\u001B[38;5;124m\"\u001B[39m, ascending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\u001B[38;5;241m.\u001B[39mshow()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:536\u001B[0m, in \u001B[0;36mDataFrame.groupBy\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m    534\u001B[0m     _cols\u001B[38;5;241m.\u001B[39mappend(c)\n\u001B[1;32m    535\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(c, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 536\u001B[0m     _cols\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m[c])\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(c, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(c, \u001B[38;5;28mbool\u001B[39m):\n\u001B[1;32m    538\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m c \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:1860\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m   1856\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconnect\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtypes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m verify_col_name\n\u001B[1;32m   1858\u001B[0m     \u001B[38;5;66;03m# Try best to verify the column name with cached schema\u001B[39;00m\n\u001B[1;32m   1859\u001B[0m     \u001B[38;5;66;03m# If fails, fall back to the server side validation\u001B[39;00m\n\u001B[0;32m-> 1860\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_col_name(item, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mschema):\n\u001B[1;32m   1861\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mselect(item)\u001B[38;5;241m.\u001B[39misLocal()\n\u001B[1;32m   1863\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_col(item)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:2000\u001B[0m, in \u001B[0;36mDataFrame.schema\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1998\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cached_schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1999\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m-> 2000\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cached_schema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mschema(query)\n\u001B[1;32m   2001\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m copy\u001B[38;5;241m.\u001B[39mdeepcopy(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cached_schema)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1262\u001B[0m, in \u001B[0;36mSparkConnectClient.schema\u001B[0;34m(self, plan)\u001B[0m\n\u001B[1;32m   1260\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m logger\u001B[38;5;241m.\u001B[39misEnabledFor(logging\u001B[38;5;241m.\u001B[39mINFO):\n\u001B[1;32m   1261\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSchema for plan: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_proto_to_string(plan,\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1262\u001B[0m schema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_analyze(method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mschema\u001B[39m\u001B[38;5;124m\"\u001B[39m, plan\u001B[38;5;241m=\u001B[39mplan)\u001B[38;5;241m.\u001B[39mschema\n\u001B[1;32m   1263\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1264\u001B[0m \u001B[38;5;66;03m# Server side should populate the struct field which is the schema.\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1540\u001B[0m, in \u001B[0;36mSparkConnectClient._analyze\u001B[0;34m(self, method, **kwargs)\u001B[0m\n\u001B[1;32m   1538\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1539\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1540\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2047\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2045\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2046\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2047\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2048\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   2049\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2149\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2134\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n\u001B[1;32m   2135\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython versions in the Spark Connect client and server are different. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2136\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo execute user-defined functions, client and server should have the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2145\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://docs.databricks.com/en/release-notes/serverless.html.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2146\u001B[0m                 )\n\u001B[1;32m   2147\u001B[0m             \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n\u001B[0;32m-> 2149\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2150\u001B[0m                 info,\n\u001B[1;32m   2151\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2152\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2153\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2154\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2156\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(status\u001B[38;5;241m.\u001B[39mmessage) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2157\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"explode(genres)\" due to data type mismatch: The first parameter requires the (\"ARRAY\" or \"MAP\") type, however \"genres\" has the type \"STRING\". SQLSTATE: 42K09;\n'Project [id#12662, title#12663, type#12664, description#12665, release_year#12666, age_certification#12667, runtime#12668, genres#12669, production_countries#12670, seasons#12671, imdb_id#12672, imdb_score#12673, imdb_votes#12674, tmdb_popularity#12675, tmdb_score#12676, explode(genres#12669) AS genre#12693]\n+- SubqueryAlias workspace.default.titles_df\n   +- Relation workspace.default.titles_df[id#12662,title#12663,type#12664,description#12665,release_year#12666,age_certification#12667,runtime#12668,genres#12669,production_countries#12670,seasons#12671,imdb_id#12672,imdb_score#12673,imdb_votes#12674,tmdb_popularity#12675,tmdb_score#12676] parquet\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:80)\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:73)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:506)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:256)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:216)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:256)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:133)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:293)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:532)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:659)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:659)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1299)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:652)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:649)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:649)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:287)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:286)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1683)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1744)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:320)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:266)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1457)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1457)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:106)\n\tat org.apache.spark.sql.Dataset.$anonfun$org$apache$spark$sql$Dataset$$withPlan$1(Dataset.scala:5054)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:5054)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1850)\n\tat org.apache.spark.sql.Dataset.$anonfun$withColumns$1(Dataset.scala:3221)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:3237)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformWithColumns(SparkConnectPlanner.scala:1516)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$7(SessionHolder.scala:614)\n\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:630)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$5(SessionHolder.scala:613)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:611)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:185)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.transformRelation$1(SparkConnectAnalyzeHandler.scala:119)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:130)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:104)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:66)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:66)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:51)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:258)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:50)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:109)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:801)\n\tat grpc_shaded.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:381)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:337)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:537)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:337)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:336)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:349)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:329)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:381)\n\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat grpc_shaded.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:351)\n\tat grpc_shaded.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:161)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:55)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:115)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:91)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:158)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:161)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "titles_df.withColumn(\"genre\", explode(\"genres\")) \\\n",
    "    .groupBy(\"genre\").avg(\"tmdb_score\").orderBy(\"avg(tmdb_score)\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a4d0d2b-deb1-4252-8828-5c9fe5c54a51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n|               title|     tmdb_popularity|\n+--------------------+--------------------+\n|            The Mire|           tt8855592|\n|        Dark Tourist|           tt8725166|\n|Back Street Girls...|           tt8693818|\n|   Sirius the Jaeger|           tt8565136|\n|The Sound of Your...|           tt7885558|\n|     Wedding Planner|           tt7885290|\n|                Ajin|           tt5537534|\n|Transformers: Rob...|           tt3604232|\n|           Swap Shop|          tt15674202|\n| The Uncanny Counter|          tt13273826|\n|           Valentino|          tt11508212|\n|       Darwin's Game|          tt11405390|\n|          Dorohedoro|          tt11147852|\n|       The Blacklist|['thriller', 'dra...|\n|            The Mole|         ['reality']|\n| Merli: Dare to Know| ['comedy', 'drama']|\n|        Robot Trains|['animation', 'ac...|\n|           One Piece|['animation', 'ac...|\n|Married at First ...|              ['US']|\n|       นางสาวก้นครัว|              ['TH']|\n+--------------------+--------------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "titles_df.filter(\"type = 'SHOW'\") \\\n",
    "    .orderBy(\"tmdb_popularity\", ascending=False).select(\"title\", \"tmdb_popularity\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcc80767-69e8-46cf-bc03-b84dc3d3e550",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Netflix",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}